<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Siddarth Narasimhan</title>

    <meta name="author" content="Siddarth Narasimhan">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Siddarth Narasimhan
                </p>
                <p>I'm an MASc student at the <a href="https://robotics.utoronto.ca/">University of Toronto Robotics Institute</a>, advised by <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Dr. Goldie Nejat</a>, where I work on building novel navigation and control policies for robots using diffusion and large language models.
                </p>
                <p>
                  I completed my BASc in <a href="https://discover.engineering.utoronto.ca/programs/engineering-programs/engineering-science/">Engineering Science</a>, Robotics Option and AI Minor at the University of Toronto with <a href="images/diploma.png">Honours</a>. In the wild, you will find me pursuing the <a href="https://en.wikipedia.org/wiki/Chess_title#National_titles">National Master title</a>, and playing the <a href="images/mrudhangam.png">mridhangam</a>.
                </p>
                <p>
                  <i>Last Updated: 09/24</i>
                </p>
                <p style="text-align:center">
                  <a href="mailto:s.narasimhan@mail.utoronto.ca">Email</a> &nbsp;/&nbsp;
                  <a href="data/Resume_2023.pdf">Resume</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/sidd-narasimhan/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=XIkuY8wAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Quest2GM/">GitHub</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/profile_picture.png"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profile_picture.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='sim2real_image'>
                    <video width=100% height=100% muted autoplay loop playsinline>
                      <source src="images/OLiVia-Nav_video.mov" type="video/mp4">
                      Your browser does not support the video tag.
                    </video>
                  </div>
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle><strong>OLiVia-Nav: An Online Lifelong Vision Language Approach for Mobile Robot Social Navigation</strong></papertitle>
                <br>
                <strong>Siddarth Narasimhan</strong>,
                <a href="https://aarontan-git.github.io/">Aaron Hao Tan</a>,
                <a href="https://jeongwoongc.github.io/">Daniel Choi</a>,
                <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
                <br>
                <em>arXiv (Submitted), 2024</em>
                <br>
                <a href="https://www.arxiv.org/abs/2409.13675">Paper</a>
                /
                <a href="https://www.youtube.com/watch?v=eyFJiOIITO0">Video</a>
                <p></p>
                <p>We introduce OLiVia-Nav, an online lifelong vision language architecture for mobile robot social navigation. By leveraging large vision-language models (VLMs) and a novel distillation process called SC-CLIP, OLiVia-Nav efficiently encodes social and environmental contexts, adapting to dynamic human environments. </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='sim2real_image'>
                    <video width=100% height=100% muted autoplay loop playsinline>
                      <source src="images/4CNet_video.mp4" type="video/mp4">
                      Your browser does not support the video tag.
                    </video>
                  </div>
                </div>
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle><strong>4CNet: A Confidence-Aware, Contrastive, Conditional, Consistency Model for Robot Map Prediction in Multi-Robot Environments</strong></papertitle>
                <br>
                <a href="https://aarontan-git.github.io/">Aaron Hao Tan</a>,
                <strong>Siddarth Narasimhan</strong>,
                <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
                <br>
                <em>arXiv (Submitted), 2024</em>
                <br>
                <a href="https://arxiv.org/abs/2402.17904">Paper</a>
                /
                <a href="https://www.youtube.com/watch?v=QtviqC-MtEM&feature=youtu.be">Video</a>
                <p></p>
                <p>We present a novel robot exploration map prediction method called Confidence-Aware Contrastive Conditional Consistency Model (4CNet), to predict (foresee) unknown spatial configurations in unknown unstructured multi- robot environments with irregularly shaped obstacles. </p>
              </td>
            </tr>

          </tbody></table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Experience</h2>
              </td>
            </tr>

            <tr></tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='sim2real_image'>
                    <video width=100% height=100% muted autoplay loop playsinline>
                      <source src="images/4CNet_video.mp4" type="video/mp4">
                      Your browser does not support the video tag.
                    </video>
                  </div>
                </div>
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle><strong>4CNet: A Confidence-Aware, Contrastive, Conditional, Consistency Model for Robot Map Prediction in Multi-Robot Environments</strong></papertitle>
                <br>
                <a href="https://aarontan-git.github.io/">Aaron Hao Tan</a>,
                <strong>Siddarth Narasimhan</strong>,
                <a href="https://scholar.google.ca/citations?user=1pCgjH0AAAAJ&hl=en">Goldie Nejat</a>
                <br>
                <em>arXiv (Submitted), 2024</em>
                <br>
                <a href="https://arxiv.org/abs/2402.17904">Paper</a>
                /
                <a href="https://www.youtube.com/watch?v=QtviqC-MtEM&feature=youtu.be">Video</a>
                <p></p>
                <p>We present a novel robot exploration map prediction method called Confidence-Aware Contrastive Conditional Consistency Model (4CNet), to predict (foresee) unknown spatial configurations in unknown unstructured multi- robot environments with irregularly shaped obstacles. </p>
              </td>
            </tr>

          </tbody></table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody></tbody>
            <tr>
              <td>
                <h2>Projects</h2>
              </td>
            </tr>

            <tr></tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='sim2real_image'>
                    <video width=100% height=100% muted autoplay loop playsinline>
                      <source src="images/informed_rrt_demo.mp4" type="video/mp4">
                      Your browser does not support the video tag.
                    </video>
                  </div>
                </div>
                </script>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle><a href="https://github.com/Quest2GM/RRT_Playground"><strong>RRT Playground</strong></a></papertitle>
                <p>An object-oriented C++ implementation of popular variants of the rapidly exploring random trees algorithm, including RRT, RRT*, Anytime RRT and Informed RRT*. </p>
              </td>
            </tr>

          </tbody></table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Thank you to <a href="https://github.com/jonbarron/jonbarron_website">John Barron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
